{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating and loading tokenizer\n",
    "#model to be downloaded from hugginface\n",
    "\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "#essentially an api request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "#api request for sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOME OPTIMISATIONS REQUIRED\n",
    "![](./images/Screenshot%20from%202022-06-30%2019-51-46.png)\n",
    "\n",
    "\n",
    "I am reaching 1.5 gb on my 6.6gb of main memory system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATING THE SENTIMENT\n",
    "    Apparently after intializing tokens for ur given sentences u can get the sentiment rating out of 5\n",
    "    5 being ver good(positive) 0 being negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sentences=[\n",
    "'Fuck this shit man',\n",
    "'Imposter syndrome hits really hard this time around',\n",
    "'I honestly love the way trump moves his ass',\n",
    "'Maybe i like this movie',\n",
    "'Fuck this dish was the best shit ever',\n",
    "'I work 40 hours a week for me to be this poor.',\n",
    "'Is it time for your medication or mine?',\n",
    "'good bad good bad good bad good bad',\n",
    "'bad bad bad good good',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test tokenizer\n",
    "tokens=tokenizer.encode(some_sentences[0],return_tensors='pt')\n",
    "\n",
    "#return tensors is set to pt for pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 69338, 10372, 24497, 10123, 10564,   102]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] fuck this shit man [SEP]'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small note\n",
    "    The tokenizer.decode() function above is not necessary for sentiment analysis, and it is to highlight the special tokens that were explained earlier in the first section. [CLS] and [SEP] are special characters representing classification and sentence separator, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4986,  0.3586, -0.9347, -1.6609, -0.6835]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4986,  0.3586, -0.9347, -1.6609, -0.6835]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Note__ :joy:\n",
    "\n",
    "\n",
    "The logits with the highest value are where the sentiment analytical value will be from. Logits is the final layer in a neural network that returns the raw values for the prediction. Logit is the tensor which the argmax function is used to return the predicted class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.argmax(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(pt.argmax(result.logits))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try to functionize this\n",
    "def sentiment_calculator(str):\n",
    "    tokens=tokenizer.encode(str,return_tensors='pt')\n",
    "    result=model(tokens)\n",
    "\n",
    "    #print out the sentiment\n",
    "    print(int(pt.argmax(result.logits))+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck this shit man\n",
      "Sentiment Analysis: 1\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Imposter syndrome hits really hard this time around\n",
      "Sentiment Analysis: 2\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "I honestly love the way trump moves his ass\n",
      "Sentiment Analysis: 5\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Maybe i like this movie\n",
      "Sentiment Analysis: 4\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Fuck this dish was the best shit ever\n",
      "Sentiment Analysis: 1\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "I work 40 hours a week for me to be this poor.\n",
      "Sentiment Analysis: 1\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Is it time for your medication or mine?\n",
      "Sentiment Analysis: 3\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "good bad good bad good bad good bad\n",
      "Sentiment Analysis: 3\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "bad bad bad good good\n",
      "Sentiment Analysis: 3\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for sen in some_sentences:\n",
    "    print(sen)\n",
    "    print(\"Sentiment Analysis: \",end=\"\")\n",
    "    sentiment_calculator(sen)\n",
    "    print(\"-_-_-_-_\"*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This thing doesnt understand slang like at all,\n",
    "Not updated with current affairs which means that trump shit is taken as positive.\n",
    "\n",
    "\n",
    "the good bad thingy was just funny\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMANCE FOOTNOTES\n",
    "\n",
    "    so the ram usage dropped to 1.1gb\n",
    "\n",
    "![](./images/Screenshot%20from%202022-06-30%2020-20-11.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Also btw this tutorial was followed from\n",
    "\n",
    "https://www.projectpro.io/article/bert-nlp-model-explained/558"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual understanding of sentences has created significant bounds in natural language processing. The continuous innovation around this subject will get even more precise in the future. These improvements can all be traced back to attention – Self-attention. \n",
    "\n",
    "This article simplifies BERT for easy understanding. We started with the sentence–BERT is a precise, huge, masked language model. Breaking down the technical terms used in the sentence has helped give a brief overview of what the model is about and what it tries to achieve. How it is created gives an insight into what happens behind the scenes, and sentiment analysis is an example of how it is used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ai_ml_nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75f149aabefc45f596bad3ecd1cce999665dc47c609d2b6ed1c0c06088326543"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
