{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 39.0/39.0 [00:00<00:00, 45.9kB/s]\n",
      "Downloading: 100%|██████████| 953/953 [00:00<00:00, 1.24MB/s]\n",
      "Downloading: 100%|██████████| 851k/851k [00:11<00:00, 76.3kB/s] \n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 144kB/s]\n"
     ]
    }
   ],
   "source": [
    "#creating and loading tokenizer\n",
    "#model to be downloaded from hugginface\n",
    "\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "#essentially an api request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 638M/638M [01:06<00:00, 10.0MB/s]  \n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "#api request for sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOME OPTIMISATIONS REQUIRED\n",
    "![](./images/Screenshot%20from%202022-06-30%2019-51-46.png)\n",
    "\n",
    "\n",
    "I am reaching 1.5 gb on my 6.6gb of main memory system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATING THE SENTIMENT\n",
    "    Apparently after intializing tokens for ur given sentences u can get the sentiment rating out of 5\n",
    "    5 being ver good(positive) 0 being negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sentences=[\n",
    "'Fuck this shit man',\n",
    "'Imposter syndrome hits really hard this time around',\n",
    "'I honestly love the way trump moves his ass',\n",
    "'Maybe i like this movie',\n",
    "'Fuck this dish was the best shit ever',\n",
    "'I work 40 hours a week for me to be this poor.',\n",
    "'Is it time for your medication or mine?',\n",
    "'good bad good bad good bad good bad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test tokenizer\n",
    "tokens=tokenizer.encode(some_sentences[0],return_tensors='pt')\n",
    "\n",
    "#return tensors is set to pt for pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 69338, 10372, 24497, 10123, 10564,   102]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] fuck this shit man [SEP]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small note\n",
    "    The tokenizer.decode() function above is not necessary for sentiment analysis, and it is to highlight the special tokens that were explained earlier in the first section. [CLS] and [SEP] are special characters representing classification and sentence separator, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.4986,  0.3586, -0.9347, -1.6609, -0.6835]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4986,  0.3586, -0.9347, -1.6609, -0.6835]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Note__ :joy:\n",
    "\n",
    "\n",
    "The logits with the highest value are where the sentiment analytical value will be from. Logits is the final layer in a neural network that returns the raw values for the prediction. Logit is the tensor which the argmax function is used to return the predicted class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.argmax(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(pt.argmax(result.logits))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try to functionize this\n",
    "def sentiment_calculator(str):\n",
    "    tokens=tokenizer.encode(str,return_tensors='pt')\n",
    "    result=model(tokens)\n",
    "\n",
    "    #print out the sentiment\n",
    "    print(int(pt.argmax(result.logits))+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck this shit man\n",
      "Sentiment Analysis: 1\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Imposter syndrome hits really hard this time around\n",
      "Sentiment Analysis: 2\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "I honestly love the way trump moves his ass\n",
      "Sentiment Analysis: 5\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Maybe i like this movie\n",
      "Sentiment Analysis: 4\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Fuck this dish was the best shit ever\n",
      "Sentiment Analysis: 1\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "I work 40 hours a week for me to be this poor.\n",
      "Sentiment Analysis: 1\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Is it time for your medication or mine?\n",
      "Sentiment Analysis: 3\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "good bad good bad good bad good bad\n",
      "Sentiment Analysis: 3\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for sen in some_sentences:\n",
    "    print(sen)\n",
    "    print(\"Sentiment Analysis: \",end=\"\")\n",
    "    sentiment_calculator(sen)\n",
    "    print(\"-_-_-_-_\"*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This thing doesnt understand slang like at all,\n",
    "Not updated with current affairs which means that trump shit is taken as positive\n",
    "\n",
    "\n",
    "the good bad thingy was just funny\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#footnotes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ai_ml_nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75f149aabefc45f596bad3ecd1cce999665dc47c609d2b6ed1c0c06088326543"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
